# MetaDrive RL Training with SAC Algorithm
Enhanced with Live Rendering, Video Recording, and Real-Time Visualization

## ğŸš— Overview

This project implements a Soft Actor-Critic (SAC) reinforcement learning agent trained in the MetaDrive Simulator, an advanced driving simulation environment for autonomous driving research.
It provides a highly visual, interactive training loop â€” including live rendering, video recording of best episodes, and real-time training analytics â€” making it suitable for both research and demonstration purposes.

## âœ¨ Key Features

- ğŸ® **Live Rendering**: Watch the agent drive and learn in real time.
- ğŸ“¹ **Automatic Video Recording**: Save high-quality videos of top-performing episodes.
- ğŸ“Š **Live Training Visualization**: Real-time plots of rewards, success rate, and episode metrics.
- ğŸ§  **TensorBoard Integration**: Track learning curves, losses, and policy metrics.
- âš™ï¸ **Parallel Training (Multi-GPU Ready)**: Utilize multiple environments to accelerate learning.
- ğŸ’¾ **Automatic Checkpointing**: Models saved periodically for safety and recovery.

## ğŸ§© Tech Stack

- **Simulation**: MetaDrive Simulator
- **RL Framework**: Stable-Baselines3 (SAC)
- **Visualization**: matplotlib, TensorBoard, imageio
- **Parallelism**: SubprocVecEnv / DummyVecEnv
- **Video Encoding**: imageio-ffmpeg

## ğŸ› ï¸ Installation

### Prerequisites

Ensure you have Python 3.8+ and CUDA (optional, for GPU acceleration).

### Install Dependencies
```bash
pip install metadrive-simulator
pip install stable-baselines3[extra]
pip install tensorboard
pip install matplotlib
pip install imageio
pip install imageio-ffmpeg
```

## ğŸš€ Usage

### 1ï¸âƒ£ Train the Agent (Basic)

Run a standard training session with parallel environments:
```bash
python train_metadrive_sac.py --n-envs 4
```

### 2ï¸âƒ£ Watch Training Live (Single Environment)

Enable real-time visualization and rendering:
```bash
python train_metadrive_sac.py --live-render --n-envs 1
```

âš ï¸ **Note**: Live rendering works best with `--n-envs 1`.

### 3ï¸âƒ£ Train Fast with Periodic Video Recording

Record every 50 episodes while using multiple environments:
```bash
python train_metadrive_sac.py --record-video --video-interval 50 --n-envs 4
```

### 4ï¸âƒ£ Enable All Visualization Tools

Combine live plots, rendering, and recording:
```bash
python train_metadrive_sac.py --visualize --live-render --record-video --n-envs 1
```

## ğŸ“ˆ Monitoring Training

### TensorBoard

All training logs are automatically saved under `./metadrive_sac_training/logs`.
To monitor in real time:
```bash
tensorboard --logdir ./metadrive_sac_training/logs
```

### Live Metrics

The Visualization Callback provides:

- Reward progression
- Episode length distribution
- Rolling success rate
- Statistical summaries

Plots are updated dynamically and saved as `training_progress.png` upon completion.

## ğŸ§ª Testing a Trained Model

After training, evaluate your agent:
```bash
python train_metadrive_sac.py --test ./metadrive_sac_training/sac_metadrive_final
```

**Options:**
```bash
--test-episodes N   # Number of test episodes (default: 10)
```

During testing:

- The agent drives in a rendered window.
- Success/failure, rewards, and episode statistics are logged.

## ğŸ“ Directory Structure
```
metadrive_sac_training/
â”‚
â”œâ”€â”€ checkpoints/        # Periodic model checkpoints
â”œâ”€â”€ logs/               # TensorBoard logs
â”œâ”€â”€ videos/             # Recorded episode videos
â”œâ”€â”€ training_progress.png  # Final training summary plot
â””â”€â”€ sac_metadrive_final.zip # Final trained model
```

## âš™ï¸ Command-Line Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--total-timesteps` | int | 1000000 | Total training steps |
| `--n-envs` | int | 8 | Number of parallel environments |
| `--live-render` | flag | - | Enable real-time environment rendering |
| `--record-video` | flag | - | Enable automatic video recording |
| `--video-interval` | int | 50 | Record every N episodes |
| `--visualize` | flag | - | Enable live matplotlib visualizations |
| `--plot-interval` | int | 200 | Update plot every N steps |
| `--device` | str | cuda | Device: cuda, cpu, or cuda:0 |
| `--test` | str | None | Path to model for evaluation |
| `--test-episodes` | int | 10 | Number of test episodes |

## ğŸ§  Architecture Overview

### Core Components

- **MetaDriveEnvWrapper** â€“ Environment subclass that manages seeds and parallel envs.
- **VideoRecorderCallback** â€“ Periodically records and saves agent performance videos.
- **VisualizationCallback** â€“ Real-time training statistics, plotting, and analytics.
- **train_sac_agent()** â€“ Central training loop integrating callbacks, TensorBoard, and checkpoints.
- **test_trained_agent()** â€“ Evaluation routine for saved policies with rendering.

## ğŸ–¥ï¸ Example Output

**During Training:**
```
[Episode 220] Reward: 45.3 | Length: 602 | Success: âœ“
Avg Reward (100): 40.7 | Success Rate (100): 62.0%
ğŸ“¹ Video saved: ./videos/best_ep220_r45.mp4
```

**TensorBoard:**

- SAC losses
- Q-values
- Entropy coefficient (Î±)
- Reward curves

**Visual Plot:**

![Training Progress](training_progress.png)

## ğŸ§¾ License

This project is distributed under the MIT License.
See [LICENSE](LICENSE) for details.

## ğŸ‘¨â€ğŸ’» Authors & Credits

Developed by Omar Baiazid

Built using:
- [MetaDrive Simulator](https://github.com/metadriverse/metadrive)
- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3)

## ğŸ§­ Next Steps

After training:
```bash
python train_metadrive_sac.py --test ./metadrive_sac_training/sac_metadrive_final
```

**Optional:**

- Convert trained SAC model to ONNX for deployment.
- Integrate reward shaping or curriculum learning.
- Explore multi-agent MetaDrive environments.

---

*"Teaching cars to drive â€” one policy gradient at a time."*
